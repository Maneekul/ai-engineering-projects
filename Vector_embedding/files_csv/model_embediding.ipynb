{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"mrp/Thai-Semantic-Textual-Similarity-Benchmark\")\n",
    "\n",
    "# Convert the test split to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset[\"test\"])\n",
    "df = df.dropna()\n",
    "# # Save as CSV\n",
    "# csv_filename = \"thai_sts_benchmark.csv\"\n",
    "# df.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# print(f\"Dataset saved as {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main-captions</th>\n",
       "      <th>MSRvid</th>\n",
       "      <th>2012test</th>\n",
       "      <th>24</th>\n",
       "      <th>2.5</th>\n",
       "      <th>ผู้หญิงมีสไตล์ผมของเธอ</th>\n",
       "      <th>ผู้หญิงแปรงผมของเธอ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>กลุ่มผู้ชายเล่นฟุตบอลบนชายหาด</td>\n",
       "      <td>กลุ่มเด็กชายกำลังเล่นฟุตบอลบนชายหาด</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>ผู้หญิงคนหนึ่งกำลังวัดข้อเท้าของผู้หญิงอีกคนหนึ่ง</td>\n",
       "      <td>ผู้หญิงวัดข้อเท้าของผู้หญิงอีกคนหนึ่ง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>ผู้ชายกำลังตัดแตงกวา</td>\n",
       "      <td>ชายคนหนึ่งกำลังหั่นแตงกวา</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>ผู้ชายกำลังเล่นพิณ</td>\n",
       "      <td>ผู้ชายกำลังเล่นแป้นพิมพ์</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>ผู้หญิงคนหนึ่งกำลังตัดหัวหอม</td>\n",
       "      <td>ผู้หญิงกำลังตัดเต้าหู้</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>main-news</td>\n",
       "      <td>MSRpar</td>\n",
       "      <td>2012train</td>\n",
       "      <td>657.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>ดังนั้นในที่อยู่ของสหภาพของเขาในเดือนมกราคมบุช...</td>\n",
       "      <td>ในเดือนมกราคมของเขา 28 ข้อความของสหภาพสหภาพบุช...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>main-news</td>\n",
       "      <td>MSRpar</td>\n",
       "      <td>2012train</td>\n",
       "      <td>666.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>สมาชิกอีก 24 คนถูกแบ่งระหว่างตัวแทนของอุตสาหกร...</td>\n",
       "      <td>ของกรรมการ 24 คนที่ไม่ได้เป็นผู้บริหารตลาดหลัก...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>main-news</td>\n",
       "      <td>MSRpar</td>\n",
       "      <td>2012train</td>\n",
       "      <td>669.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>สังฆมณฑล Episcopal ของ Central Florida กลายเป็...</td>\n",
       "      <td>สังฆมณฑล Episcopal แห่งเซ็นทรัลฟลอริด้าโหวตวัน...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>main-news</td>\n",
       "      <td>MSRpar</td>\n",
       "      <td>2012train</td>\n",
       "      <td>679.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>McGill ยังมีรายละเอียดหลุมที่ถูกตัดในลำตัวของ ...</td>\n",
       "      <td>McGill ยังกล่าวอีกว่าถุงมือดำถูกยัดเข้าไปในหลุ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>main-news</td>\n",
       "      <td>MSRpar</td>\n",
       "      <td>2012train</td>\n",
       "      <td>693.0</td>\n",
       "      <td>3.20</td>\n",
       "      <td>เมื่อเทียบกับกำไรปีก่อนหน้า 102 ล้านดอลลาร์หรื...</td>\n",
       "      <td>นั่นเป็นมากกว่าสองเท่าของ $ 102 ล้านหรือ 13 เซ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1117 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      main-captions  MSRvid   2012test     24   2.5  \\\n",
       "0     main-captions  MSRvid   2012test   33.0  3.60   \n",
       "1     main-captions  MSRvid   2012test   45.0  5.00   \n",
       "2     main-captions  MSRvid   2012test   63.0  4.20   \n",
       "3     main-captions  MSRvid   2012test   66.0  1.50   \n",
       "4     main-captions  MSRvid   2012test   74.0  1.80   \n",
       "...             ...     ...        ...    ...   ...   \n",
       "1112      main-news  MSRpar  2012train  657.0  4.00   \n",
       "1113      main-news  MSRpar  2012train  666.0  4.00   \n",
       "1114      main-news  MSRpar  2012train  669.0  2.75   \n",
       "1115      main-news  MSRpar  2012train  679.0  2.25   \n",
       "1116      main-news  MSRpar  2012train  693.0  3.20   \n",
       "\n",
       "                                 ผู้หญิงมีสไตล์ผมของเธอ  \\\n",
       "0                         กลุ่มผู้ชายเล่นฟุตบอลบนชายหาด   \n",
       "1     ผู้หญิงคนหนึ่งกำลังวัดข้อเท้าของผู้หญิงอีกคนหนึ่ง   \n",
       "2                                  ผู้ชายกำลังตัดแตงกวา   \n",
       "3                                    ผู้ชายกำลังเล่นพิณ   \n",
       "4                          ผู้หญิงคนหนึ่งกำลังตัดหัวหอม   \n",
       "...                                                 ...   \n",
       "1112  ดังนั้นในที่อยู่ของสหภาพของเขาในเดือนมกราคมบุช...   \n",
       "1113  สมาชิกอีก 24 คนถูกแบ่งระหว่างตัวแทนของอุตสาหกร...   \n",
       "1114  สังฆมณฑล Episcopal ของ Central Florida กลายเป็...   \n",
       "1115  McGill ยังมีรายละเอียดหลุมที่ถูกตัดในลำตัวของ ...   \n",
       "1116  เมื่อเทียบกับกำไรปีก่อนหน้า 102 ล้านดอลลาร์หรื...   \n",
       "\n",
       "                                    ผู้หญิงแปรงผมของเธอ  \n",
       "0                   กลุ่มเด็กชายกำลังเล่นฟุตบอลบนชายหาด  \n",
       "1                 ผู้หญิงวัดข้อเท้าของผู้หญิงอีกคนหนึ่ง  \n",
       "2                             ชายคนหนึ่งกำลังหั่นแตงกวา  \n",
       "3                              ผู้ชายกำลังเล่นแป้นพิมพ์  \n",
       "4                                ผู้หญิงกำลังตัดเต้าหู้  \n",
       "...                                                 ...  \n",
       "1112  ในเดือนมกราคมของเขา 28 ข้อความของสหภาพสหภาพบุช...  \n",
       "1113  ของกรรมการ 24 คนที่ไม่ได้เป็นผู้บริหารตลาดหลัก...  \n",
       "1114  สังฆมณฑล Episcopal แห่งเซ็นทรัลฟลอริด้าโหวตวัน...  \n",
       "1115  McGill ยังกล่าวอีกว่าถุงมือดำถูกยัดเข้าไปในหลุ...  \n",
       "1116  นั่นเป็นมากกว่าสองเท่าของ $ 102 ล้านหรือ 13 เซ...  \n",
       "\n",
       "[1117 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch(texts):\n",
    "    url = \"https://ai-api.manageai.co.th/text-embedding-01/embed_all\"\n",
    "    payload = json.dumps({\n",
    "        \"inputs\": texts,  # Send multiple texts in one batch\n",
    "        \"normalize\": True,\n",
    "        \"truncate\": False,\n",
    "        \"truncation_direction\": \"Right\"\n",
    "    })\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Basic bWFuYWdlYWkyMDI0Ok1hbmFnZUFJQDIwMjQ='\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make the POST request to the API\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        response.raise_for_status()  # Raise an error for 4xx/5xx status codes\n",
    "\n",
    "        # Print the raw response text to see the details of the response\n",
    "        # print(\"response.text ==> \",response.text)  # This will print the raw JSON response\n",
    "\n",
    "        # Parse the response JSON\n",
    "        data = response.json()\n",
    "\n",
    "        # Print the parsed JSON data to inspect it\n",
    "        # print(\"data ==> \",data)  # This will print the parsed JSON data\n",
    "\n",
    "        # Return the embedding data\n",
    "        return [np.array(d[0], dtype=np.float32) for d in data] if data else None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def batch_embed_dataframe(df, columns_to_embed, batch_size=10):\n",
    "    embeddings = {col: [] for col in columns_to_embed}\n",
    "\n",
    "    for col in columns_to_embed:\n",
    "        tqdm.pandas(desc=f\"Embedding {col}\")\n",
    "        \n",
    "        # Split the column into batches and process each batch\n",
    "        for start in tqdm(range(0, len(df), batch_size)):\n",
    "            end = min(start + batch_size, len(df))\n",
    "            texts_batch = df[col].iloc[start:end].dropna().tolist()\n",
    "            embeddings_batch = get_embeddings_batch(texts_batch)\n",
    "            \n",
    "            # Store the embeddings in the corresponding list\n",
    "            if embeddings_batch:\n",
    "                embeddings[col].extend(embeddings_batch)\n",
    "            else:\n",
    "                embeddings[col].extend([None] * (end - start))  # If embedding failed, store None\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [02:51<00:00,  1.53s/it]\n",
      "100%|██████████| 112/112 [03:42<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding completed in batches!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Assuming your DataFrame is named `test`\n",
    "columns_to_embed = ['ผู้หญิงมีสไตล์ผมของเธอ','ผู้หญิงแปรงผมของเธอ']\n",
    "embedding_results = batch_embed_dataframe(df, columns_to_embed)\n",
    "\n",
    "# Add the embeddings to the dataframe\n",
    "for col in columns_to_embed:\n",
    "    df[f\"{col}_embedding\"] = embedding_results[col]\n",
    "\n",
    "print(\"✅ Embedding completed in batches!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = df['ผู้หญิงมีสไตล์ผมของเธอ_embedding'].to_list()\n",
    "embeddings2 = df['ผู้หญิงแปรงผมของเธอ_embedding'].to_list()\n",
    "\n",
    "similarity_scores = [cosine_similarity([embeddings1[i]], [embeddings2[i]])[0][0] for i in range(len(embeddings1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cosine_similarity\"] = similarity_scores\n",
    "df[\"rescaled_similarity\"] = df[\"cosine_similarity\"] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_similarity = df['2.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Correlation (*100): 76.47\n"
     ]
    }
   ],
   "source": [
    "spearman_corr, _ = spearmanr(df[\"rescaled_similarity\"], ground_truth_similarity)\n",
    "# คูณ 100 เพื่อให้เป็นเปอร์เซ็นต์\n",
    "spearman_corr_percent = spearman_corr * 100\n",
    "\n",
    "# แสดงผล\n",
    "print(f\"Spearman's Correlation (*100): {spearman_corr_percent:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paraphrase-multilingual-mpnet-base-v2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(r'C:\\Users\\kantaphong\\Desktop\\Work Station\\medium\\vector_embedding\\files_csv\\thai_sts_benchmark.csv')\n",
    "dataset_df = dataset_df.dropna()\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "sentences1 = dataset_df['ผู้หญิงมีสไตล์ผมของเธอ'].tolist()\n",
    "sentences2 = dataset_df['ผู้หญิงแปรงผมของเธอ'].tolist()\n",
    "\n",
    "# Generate sentence embeddings\n",
    "embeddings1 = model.encode(sentences1)\n",
    "embeddings2 = model.encode(sentences2)\n",
    "\n",
    "# Add embeddings to DataFrame (optional)\n",
    "dataset_df['embedding1'] = embeddings1.tolist()\n",
    "dataset_df['embedding2'] = embeddings2.tolist()\n",
    "\n",
    "similarity_scores = cosine_similarity(embeddings1, embeddings2)\n",
    "dataset_df['cosine_similarity'] = [similarity_scores[i][i] for i in range(len(similarity_scores))]\n",
    "\n",
    "average_similarity = dataset_df['cosine_similarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Correlation (*100): 80.16\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# โหลดชุดข้อมูล\n",
    "dataset = load_dataset(\"mrp/Thai-Semantic-Textual-Similarity-Benchmark\")\n",
    "\n",
    "# โหลดโมเดล SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# แปลงเป็น DataFrame\n",
    "df = pd.DataFrame(dataset[\"test\"])\n",
    "df = df.dropna()\n",
    "\n",
    "# ดึงข้อความจากคอลัมน์ sentence1 และ sentence2\n",
    "sentences1 = df['ผู้หญิงมีสไตล์ผมของเธอ'].tolist()\n",
    "sentences2 = df['ผู้หญิงแปรงผมของเธอ'].tolist()\n",
    "\n",
    "# ดึง ground truth similarity (assumes column name is 'score')\n",
    "ground_truth_similarity = df['2.5'].tolist()\n",
    "\n",
    "# สร้าง embeddings\n",
    "embeddings1 = model.encode(sentences1, convert_to_numpy=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_numpy=True)\n",
    "\n",
    "# คำนวณ Cosine Similarity\n",
    "similarity_scores = [cosine_similarity([embeddings1[i]], [embeddings2[i]])[0][0] for i in range(len(embeddings1))]\n",
    "\n",
    "\n",
    "# เพิ่มค่าความคล้ายคลึงลงใน DataFrame\n",
    "df[\"cosine_similarity\"] = similarity_scores\n",
    "# Rescale cosine similarity\n",
    "df[\"rescaled_similarity\"] = df[\"cosine_similarity\"] * 5\n",
    "\n",
    "# คำนวณ Spearman’s Correlation ระหว่างค่าความคล้ายคลึงที่ได้จากโมเดล และ ground truth\n",
    "spearman_corr, _ = spearmanr(df[\"rescaled_similarity\"], ground_truth_similarity)\n",
    "\n",
    "# คูณ 100 เพื่อให้เป็นเปอร์เซ็นต์\n",
    "spearman_corr_percent = spearman_corr * 100\n",
    "\n",
    "# แสดงผล\n",
    "print(f\"Spearman's Correlation (*100): {spearman_corr_percent:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huggungface ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from datasets import load_dataset\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from scipy.stats import spearmanr\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = load_dataset(\"mrp/Thai-Semantic-Textual-Similarity-Benchmark\")\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # Mean pooling function\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# # Convert the test split to a pandas DataFrame\n",
    "# df = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "# # Extract the sentence pairs (assuming columns are \"sentence1\" and \"sentence2\")\n",
    "# sentences1 = dataset_df['ผู้หญิงมีสไตล์ผมของเธอ'].tolist()\n",
    "# sentences2 = dataset_df['ผู้หญิงแปรงผมของเธอ'].tolist()\n",
    "\n",
    "# # Extract the ground truth similarity scores (assumes column name is 'score')\n",
    "# ground_truth_similarity = df['score'].tolist()  # Use the correct column that has the similarity score\n",
    "\n",
    "# # Tokenize the sentences\n",
    "# encoding1 = tokenizer(sentences1, padding=True, truncation=True, return_tensors='pt')\n",
    "# encoding2 = tokenizer(sentences2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Generate sentence embeddings using mean pooling\n",
    "# with torch.no_grad():\n",
    "#     model_output1 = model(**encoding1)\n",
    "#     model_output2 = model(**encoding2)\n",
    "    \n",
    "#     embeddings1 = mean_pooling(model_output1, encoding1['attention_mask'])\n",
    "#     embeddings2 = mean_pooling(model_output2, encoding2['attention_mask'])\n",
    "\n",
    "# # Calculate cosine similarity\n",
    "# similarity_scores = cosine_similarity(embeddings1.numpy(), embeddings2.numpy())\n",
    "\n",
    "# # Add similarity scores to DataFrame\n",
    "# df['cosine_similarity'] = [similarity_scores[i][i] for i in range(len(similarity_scores))]\n",
    "\n",
    "# # Calculate Spearman's Correlation (*100)\n",
    "# spearman_corr, _ = spearmanr(df['cosine_similarity'], ground_truth_similarity)\n",
    "\n",
    "# # Convert to percentage (*100)\n",
    "# spearman_corr_percent = spearman_corr * 100\n",
    "\n",
    "# # Print the Spearman's correlation\n",
    "# print(f\"Spearman's Correlation (*100): {spearman_corr_percent:.2f}\")\n",
    "\n",
    "# # Optionally, calculate the average cosine similarity\n",
    "# average_similarity = df['cosine_similarity'].mean()\n",
    "# print(f\"Average Cosine Similarity: {average_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_bge = pd.read_csv(r'C:\\Users\\kantaphong\\Desktop\\Work Station\\medium\\vector_embedding\\files_csv\\sts-test_th.csv')\n",
    "dataset_df_bge = dataset_df_bge.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_bge2 = pd.read_csv(r'C:\\Users\\kantaphong\\Desktop\\Work Station\\medium\\vector_embedding\\files_csv\\sts-test_th.csv')\n",
    "dataset_df_bge2 = dataset_df_bge2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ผู้หญิงกำลังจัดแต่งทรงผมของเธอ</th>\n",
       "      <th>ผู้หญิงคนหนึ่งกำลังแปรงผมของเธอ</th>\n",
       "      <th>2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>กลุ่มผู้ชายเล่นฟุตบอลบนชายหาด</td>\n",
       "      <td>กลุ่มเด็กผู้ชายกำลังเล่นฟุตบอลบนชายหาด</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ผู้หญิงคนหนึ่งกำลังวัดข้อเท้าของผู้หญิงคนอื่น</td>\n",
       "      <td>ผู้หญิงวัดข้อเท้าของผู้หญิงคนอื่น</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ผู้ชายกำลังตัดแตงกวา</td>\n",
       "      <td>ผู้ชายกำลังหั่นแตงกวา</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ผู้ชายกำลังเล่นพิณ</td>\n",
       "      <td>ผู้ชายกำลังเล่นแป้นพิมพ์</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ผู้หญิงกำลังตัดหัวหอม</td>\n",
       "      <td>ผู้หญิงกำลังตัดเต้าหู้</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>ฟิลิปปินส์แคนาดาให้คำมั่นว่าจะเพิ่มความสัมพันธ...</td>\n",
       "      <td>ฟิลิปปินส์ช่วยประหยัด 100 หลังจมูก</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>ชาวอิสราเอลบาร์ชาวปาเลสไตน์จากเมืองเก่าของเยรู...</td>\n",
       "      <td>การแก้ปัญหาสองรัฐระหว่างชาวปาเลสไตน์อิสราเอลพา...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>คุณรู้เกี่ยวกับหน่วยสืบราชการลับมากแค่ไหน?</td>\n",
       "      <td>ผู้ร่างกฎหมายจากทั้งสองฝ่ายแสดงความไม่พอใจที่ห...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>โอบามาดิ้นรนเพื่อบรรเทาความกลัวของซาอุดิอาระเบ...</td>\n",
       "      <td>พม่าพยายามดิ้นรนเพื่อสรุปรายชื่อผู้มีสิทธิเลือ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>เกาหลีใต้ประกาศว่าการระบาดของโรคจะสิ้นสุดลง</td>\n",
       "      <td>คณะผู้แทนเกาหลีเหนือพบกับเจ้าหน้าที่เกาหลีใต้</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ผู้หญิงกำลังจัดแต่งทรงผมของเธอ  \\\n",
       "0                         กลุ่มผู้ชายเล่นฟุตบอลบนชายหาด   \n",
       "1         ผู้หญิงคนหนึ่งกำลังวัดข้อเท้าของผู้หญิงคนอื่น   \n",
       "2                                  ผู้ชายกำลังตัดแตงกวา   \n",
       "3                                    ผู้ชายกำลังเล่นพิณ   \n",
       "4                                 ผู้หญิงกำลังตัดหัวหอม   \n",
       "...                                                 ...   \n",
       "1373  ฟิลิปปินส์แคนาดาให้คำมั่นว่าจะเพิ่มความสัมพันธ...   \n",
       "1374  ชาวอิสราเอลบาร์ชาวปาเลสไตน์จากเมืองเก่าของเยรู...   \n",
       "1375         คุณรู้เกี่ยวกับหน่วยสืบราชการลับมากแค่ไหน?   \n",
       "1376  โอบามาดิ้นรนเพื่อบรรเทาความกลัวของซาอุดิอาระเบ...   \n",
       "1377        เกาหลีใต้ประกาศว่าการระบาดของโรคจะสิ้นสุดลง   \n",
       "\n",
       "                        ผู้หญิงคนหนึ่งกำลังแปรงผมของเธอ  2.5  \n",
       "0                กลุ่มเด็กผู้ชายกำลังเล่นฟุตบอลบนชายหาด  3.6  \n",
       "1                     ผู้หญิงวัดข้อเท้าของผู้หญิงคนอื่น  5.0  \n",
       "2                                 ผู้ชายกำลังหั่นแตงกวา  4.2  \n",
       "3                              ผู้ชายกำลังเล่นแป้นพิมพ์  1.5  \n",
       "4                                ผู้หญิงกำลังตัดเต้าหู้  1.8  \n",
       "...                                                 ...  ...  \n",
       "1373                 ฟิลิปปินส์ช่วยประหยัด 100 หลังจมูก  0.0  \n",
       "1374  การแก้ปัญหาสองรัฐระหว่างชาวปาเลสไตน์อิสราเอลพา...  1.0  \n",
       "1375  ผู้ร่างกฎหมายจากทั้งสองฝ่ายแสดงความไม่พอใจที่ห...  1.0  \n",
       "1376  พม่าพยายามดิ้นรนเพื่อสรุปรายชื่อผู้มีสิทธิเลือ...  0.0  \n",
       "1377      คณะผู้แทนเกาหลีเหนือพบกับเจ้าหน้าที่เกาหลีใต้  0.0  \n",
       "\n",
       "[1378 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df_bge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = dataset_df_bge['ผู้หญิงกำลังจัดแต่งทรงผมของเธอ'].tolist()\n",
    "sentences2 = dataset_df_bge['ผู้หญิงคนหนึ่งกำลังแปรงผมของเธอ'].tolist()\n",
    "\n",
    "ground_truth_similarity = dataset_df_bge['2.5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████| 115/115 [00:00<00:00, 2620.04it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|██████████| 115/115 [00:01<00:00, 93.84it/s] \n",
      "pre tokenize: 100%|██████████| 115/115 [00:00<00:00, 3031.52it/s]\n",
      "Inference Embeddings: 100%|██████████| 115/115 [00:01<00:00, 101.09it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(sentences1, \n",
    "                            batch_size=12, \n",
    "                            max_length=128, \n",
    "                            )['dense_vecs']\n",
    "\n",
    "embeddings_2 = model.encode(sentences2,\n",
    "                            batch_size=12, \n",
    "                            max_length=128,\n",
    "                            )['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = [cosine_similarity([embeddings_1[i]], [embeddings_2[i]])[0][0] for i in range(len(embeddings_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_bge[\"cosine_similarity\"] = similarity_scores\n",
    "dataset_df_bge[\"rescaled_similarity\"] = dataset_df_bge[\"cosine_similarity\"] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Correlation (*100): 77.26\n"
     ]
    }
   ],
   "source": [
    "spearman_corr, _ = spearmanr(dataset_df_bge[\"rescaled_similarity\"], ground_truth_similarity)\n",
    "\n",
    "# คูณ 100 เพื่อให้เป็นเปอร์เซ็นต์\n",
    "spearman_corr_percent = spearman_corr * 100\n",
    "\n",
    "# แสดงผล\n",
    "print(f\"Spearman's Correlation (*100): {spearman_corr_percent:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alibaba-NLP/gte-Qwen2-1.5B-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 32768, 'do_lower_case': False}) with Transformer model: Qwen2Model \n",
       "  (1): Pooling({'word_embedding_dimension': 1536, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': True, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "model.to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_Qwen2_1_5B = pd.read_csv(r'C:\\Users\\kantaphong\\Desktop\\Work Station\\medium\\vector_embedding\\files_csv\\thai_sts_benchmark.csv')\n",
    "dataset_df_Qwen2_1_5B = dataset_df_Qwen2_1_5B.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "# In case you want to reduce the maximum length:\n",
    "model.max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentences on GPU\n",
    "sentences1 = dataset_df_Qwen2_1_5B['ผู้หญิงมีสไตล์ผมของเธอ'].tolist()\n",
    "sentences2 = dataset_df_Qwen2_1_5B['ผู้หญิงแปรงผมของเธอ'].tolist()\n",
    "\n",
    "ground_truth_similarity = dataset_df_Qwen2_1_5B['2.5'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sentences2)\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:393\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    391\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 393\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\Alibaba-NLP\\gte-Qwen2-1.5B-instruct\\0d2ad8e1ac654a2b626e62154778a70868141208\\modeling_qwen.py:1081\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels, is_causal)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1071\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1072\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1078\u001b[0m         is_causal,\n\u001b[0;32m   1079\u001b[0m     )\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1081\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\Alibaba-NLP\\gte-Qwen2-1.5B-instruct\\0d2ad8e1ac654a2b626e62154778a70868141208\\modeling_qwen.py:777\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, is_causal, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;124;03m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    775\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 777\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m    780\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    781\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    782\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    787\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    788\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query_embeddings = model.encode(sentences1, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(sentences2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1117, 1117)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences1) , len(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embeddings = model.encode(sentences1, prompt_name=\"query\", device=device)\n",
    "# document_embeddings = model.encode(sentences2, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1117, 1117)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_embeddings) , len(query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = [cosine_similarity([query_embeddings[i]], [document_embeddings[i]])[0][0] for i in range(len(query_embeddings))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_Qwen2_1_5B[\"cosine_similarity\"] = similarity_scores\n",
    "dataset_df_Qwen2_1_5B[\"rescaled_similarity\"] = dataset_df_Qwen2_1_5B[\"cosine_similarity\"] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's Correlation (*100): 70.20\n"
     ]
    }
   ],
   "source": [
    "spearman_corr, _ = spearmanr(dataset_df_Qwen2_1_5B[\"rescaled_similarity\"], ground_truth_similarity)\n",
    "\n",
    "# คูณ 100 เพื่อให้เป็นเปอร์เซ็นต์\n",
    "spearman_corr_percent = spearman_corr * 100\n",
    "\n",
    "# แสดงผล\n",
    "print(f\"Spearman's Correlation (*100): {spearman_corr_percent:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alibaba-NLP/gte-Qwen2-7B-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]c:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kantaphong\\.cache\\huggingface\\hub\\models--Alibaba-NLP--gte-Qwen2-7B-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 7/7 [28:03<00:00, 240.45s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:03<00:00,  1.87it/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct:\n",
      "- tokenization_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 18.23 GiB is allocated by PyTorch, and 103.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlibaba-NLP/gte-Qwen2-7B-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model.to(device)  # Move model to GPU\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:347\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 900 (3 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kantaphong\\anaconda3\\envs\\llm-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 18.23 GiB is allocated by PyTorch, and 103.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", trust_remote_code=True)\n",
    "# model.to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
